{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e9573c2e35c44e",
   "metadata": {},
   "source": [
    "# Power Flow Neural Network Training with Pandapower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e29ffe2e7920de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:30:36.793326Z",
     "start_time": "2025-01-21T14:30:26.828344Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter # for pytorch visualization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # normalize input features and target values\n",
    "from sklearn.model_selection import ParameterGrid # for hyperparameter tuning\n",
    "\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c544dc9ac3f11ff",
   "metadata": {},
   "source": [
    "### 1. Import Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31f12038d1d4aaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:35:35.365251Z",
     "start_time": "2025-01-21T14:35:35.326547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./vector_data_solution_prediction.npy')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fb10ff26e6c08",
   "metadata": {},
   "source": [
    "### 2. Create Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b780979cebdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(DeepNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917333691c8b907",
   "metadata": {},
   "source": [
    "### 3. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bed112667f7a0282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- BEST MODEL - Epoch 0 - Val Loss 0.215076 -------------\n",
      "Epoch 000, Loss: 0.253147, Val Loss: 0.215076\n",
      "------------- BEST MODEL - Epoch 1 - Val Loss 0.098882 -------------\n",
      "------------- BEST MODEL - Epoch 2 - Val Loss 0.086900 -------------\n",
      "------------- BEST MODEL - Epoch 3 - Val Loss 0.082290 -------------\n",
      "------------- BEST MODEL - Epoch 4 - Val Loss 0.077584 -------------\n",
      "------------- BEST MODEL - Epoch 5 - Val Loss 0.071921 -------------\n",
      "------------- BEST MODEL - Epoch 6 - Val Loss 0.066185 -------------\n",
      "------------- BEST MODEL - Epoch 7 - Val Loss 0.061007 -------------\n",
      "------------- BEST MODEL - Epoch 8 - Val Loss 0.057656 -------------\n",
      "------------- BEST MODEL - Epoch 9 - Val Loss 0.048979 -------------\n",
      "------------- BEST MODEL - Epoch 10 - Val Loss 0.044682 -------------\n",
      "Epoch 010, Loss: 0.054665, Val Loss: 0.044682\n",
      "------------- BEST MODEL - Epoch 11 - Val Loss 0.039062 -------------\n",
      "------------- BEST MODEL - Epoch 12 - Val Loss 0.032405 -------------\n",
      "------------- BEST MODEL - Epoch 13 - Val Loss 0.028079 -------------\n",
      "------------- BEST MODEL - Epoch 14 - Val Loss 0.024451 -------------\n",
      "------------- BEST MODEL - Epoch 15 - Val Loss 0.020764 -------------\n",
      "------------- BEST MODEL - Epoch 16 - Val Loss 0.016203 -------------\n",
      "------------- BEST MODEL - Epoch 17 - Val Loss 0.013884 -------------\n",
      "------------- BEST MODEL - Epoch 18 - Val Loss 0.012543 -------------\n",
      "------------- BEST MODEL - Epoch 19 - Val Loss 0.011542 -------------\n",
      "------------- BEST MODEL - Epoch 20 - Val Loss 0.010728 -------------\n",
      "Epoch 020, Loss: 0.013010, Val Loss: 0.010728\n",
      "------------- BEST MODEL - Epoch 21 - Val Loss 0.010273 -------------\n",
      "------------- BEST MODEL - Epoch 22 - Val Loss 0.010003 -------------\n",
      "------------- BEST MODEL - Epoch 24 - Val Loss 0.009863 -------------\n",
      "------------- BEST MODEL - Epoch 25 - Val Loss 0.009739 -------------\n",
      "------------- BEST MODEL - Epoch 26 - Val Loss 0.009447 -------------\n",
      "------------- BEST MODEL - Epoch 29 - Val Loss 0.009310 -------------\n",
      "Epoch 030, Loss: 0.009855, Val Loss: 0.009608\n",
      "------------- BEST MODEL - Epoch 34 - Val Loss 0.009219 -------------\n",
      "------------- BEST MODEL - Epoch 35 - Val Loss 0.009154 -------------\n",
      "------------- BEST MODEL - Epoch 36 - Val Loss 0.009051 -------------\n",
      "Epoch 040, Loss: 0.010645, Val Loss: 0.009126\n",
      "------------- BEST MODEL - Epoch 44 - Val Loss 0.009037 -------------\n",
      "------------- BEST MODEL - Epoch 45 - Val Loss 0.008876 -------------\n",
      "Epoch 050, Loss: 0.010940, Val Loss: 0.009366\n",
      "------------- BEST MODEL - Epoch 52 - Val Loss 0.008815 -------------\n",
      "------------- BEST MODEL - Epoch 54 - Val Loss 0.008572 -------------\n",
      "------------- BEST MODEL - Epoch 55 - Val Loss 0.008461 -------------\n",
      "Epoch 060, Loss: 0.007119, Val Loss: 0.009500\n",
      "------------- BEST MODEL - Epoch 65 - Val Loss 0.008369 -------------\n",
      "------------- BEST MODEL - Epoch 70 - Val Loss 0.008238 -------------\n",
      "Epoch 070, Loss: 0.008544, Val Loss: 0.008238\n",
      "------------- BEST MODEL - Epoch 74 - Val Loss 0.008044 -------------\n",
      "------------- BEST MODEL - Epoch 75 - Val Loss 0.007820 -------------\n",
      "------------- BEST MODEL - Epoch 79 - Val Loss 0.007592 -------------\n",
      "Epoch 080, Loss: 0.006547, Val Loss: 0.008320\n",
      "------------- BEST MODEL - Epoch 81 - Val Loss 0.007496 -------------\n",
      "------------- BEST MODEL - Epoch 84 - Val Loss 0.007017 -------------\n",
      "------------- BEST MODEL - Epoch 89 - Val Loss 0.006297 -------------\n",
      "Epoch 090, Loss: 0.004659, Val Loss: 0.006353\n",
      "------------- BEST MODEL - Epoch 91 - Val Loss 0.005982 -------------\n",
      "------------- BEST MODEL - Epoch 94 - Val Loss 0.005471 -------------\n",
      "------------- BEST MODEL - Epoch 95 - Val Loss 0.005340 -------------\n",
      "------------- BEST MODEL - Epoch 96 - Val Loss 0.005307 -------------\n",
      "------------- BEST MODEL - Epoch 97 - Val Loss 0.005024 -------------\n",
      "------------- BEST MODEL - Epoch 98 - Val Loss 0.004738 -------------\n",
      "------------- BEST MODEL - Epoch 100 - Val Loss 0.004406 -------------\n",
      "Epoch 100, Loss: 0.002264, Val Loss: 0.004406\n",
      "------------- BEST MODEL - Epoch 104 - Val Loss 0.004038 -------------\n",
      "------------- BEST MODEL - Epoch 108 - Val Loss 0.003558 -------------\n",
      "Epoch 110, Loss: 0.001780, Val Loss: 0.005150\n",
      "------------- BEST MODEL - Epoch 112 - Val Loss 0.003517 -------------\n",
      "------------- BEST MODEL - Epoch 115 - Val Loss 0.003199 -------------\n",
      "------------- BEST MODEL - Epoch 116 - Val Loss 0.003078 -------------\n",
      "Epoch 120, Loss: 0.001197, Val Loss: 0.003080\n",
      "------------- BEST MODEL - Epoch 121 - Val Loss 0.002894 -------------\n",
      "------------- BEST MODEL - Epoch 123 - Val Loss 0.002823 -------------\n",
      "------------- BEST MODEL - Epoch 127 - Val Loss 0.002589 -------------\n",
      "Epoch 130, Loss: 0.002375, Val Loss: 0.005788\n",
      "------------- BEST MODEL - Epoch 133 - Val Loss 0.002468 -------------\n",
      "Epoch 140, Loss: 0.001850, Val Loss: 0.004039\n",
      "------------- BEST MODEL - Epoch 143 - Val Loss 0.002201 -------------\n",
      "Epoch 150, Loss: 0.001572, Val Loss: 0.002440\n",
      "------------- BEST MODEL - Epoch 154 - Val Loss 0.002126 -------------\n",
      "------------- BEST MODEL - Epoch 158 - Val Loss 0.002085 -------------\n",
      "Epoch 160, Loss: 0.001364, Val Loss: 0.002948\n",
      "------------- BEST MODEL - Epoch 163 - Val Loss 0.001984 -------------\n",
      "Epoch 170, Loss: 0.003960, Val Loss: 0.004811\n",
      "------------- BEST MODEL - Epoch 171 - Val Loss 0.001953 -------------\n",
      "------------- BEST MODEL - Epoch 174 - Val Loss 0.001946 -------------\n",
      "Epoch 180, Loss: 0.002656, Val Loss: 0.004066\n",
      "------------- BEST MODEL - Epoch 181 - Val Loss 0.001900 -------------\n",
      "------------- BEST MODEL - Epoch 188 - Val Loss 0.001838 -------------\n",
      "Epoch 190, Loss: 0.001855, Val Loss: 0.002518\n",
      "Epoch 200, Loss: 0.002930, Val Loss: 0.004833\n",
      "Epoch 210, Loss: 0.004577, Val Loss: 0.006248\n",
      "Epoch 220, Loss: 0.001977, Val Loss: 0.002164\n",
      "Epoch 230, Loss: 0.001707, Val Loss: 0.002130\n",
      "Epoch 240, Loss: 0.001282, Val Loss: 0.001865\n",
      "------------- BEST MODEL - Epoch 250 - Val Loss 0.001829 -------------\n",
      "Epoch 250, Loss: 0.001631, Val Loss: 0.001829\n",
      "------------- BEST MODEL - Epoch 253 - Val Loss 0.001819 -------------\n",
      "Epoch 260, Loss: 0.002008, Val Loss: 0.002415\n",
      "Epoch 270, Loss: 0.001752, Val Loss: 0.001968\n",
      "Epoch 280, Loss: 0.001859, Val Loss: 0.002745\n",
      "Epoch 290, Loss: 0.001367, Val Loss: 0.001871\n",
      "------------- BEST MODEL - Epoch 293 - Val Loss 0.001748 -------------\n",
      "Epoch 300, Loss: 0.001850, Val Loss: 0.001899\n",
      "------------- BEST MODEL - Epoch 305 - Val Loss 0.001681 -------------\n",
      "Epoch 310, Loss: 0.001971, Val Loss: 0.002158\n",
      "------------- BEST MODEL - Epoch 312 - Val Loss 0.001631 -------------\n",
      "Epoch 320, Loss: 0.003546, Val Loss: 0.003811\n",
      "Epoch 330, Loss: 0.003085, Val Loss: 0.004124\n",
      "------------- BEST MODEL - Epoch 336 - Val Loss 0.001625 -------------\n",
      "Epoch 340, Loss: 0.001788, Val Loss: 0.001682\n",
      "Epoch 350, Loss: 0.002125, Val Loss: 0.001827\n",
      "------------- BEST MODEL - Epoch 357 - Val Loss 0.001610 -------------\n",
      "Epoch 360, Loss: 0.001457, Val Loss: 0.001845\n",
      "------------- BEST MODEL - Epoch 367 - Val Loss 0.001599 -------------\n",
      "Epoch 370, Loss: 0.002734, Val Loss: 0.002896\n",
      "Epoch 380, Loss: 0.002045, Val Loss: 0.002670\n",
      "Epoch 390, Loss: 0.001917, Val Loss: 0.001611\n",
      "------------- BEST MODEL - Epoch 400 - Val Loss 0.001531 -------------\n",
      "Epoch 400, Loss: 0.001707, Val Loss: 0.001531\n",
      "------------- BEST MODEL - Epoch 402 - Val Loss 0.001499 -------------\n",
      "Epoch 410, Loss: 0.002013, Val Loss: 0.002220\n",
      "Epoch 420, Loss: 0.001880, Val Loss: 0.001596\n",
      "Epoch 430, Loss: 0.002422, Val Loss: 0.001999\n",
      "------------- BEST MODEL - Epoch 434 - Val Loss 0.001478 -------------\n",
      "Epoch 440, Loss: 0.001807, Val Loss: 0.001748\n",
      "Epoch 450, Loss: 0.001669, Val Loss: 0.001597\n",
      "------------- BEST MODEL - Epoch 460 - Val Loss 0.001478 -------------\n",
      "Epoch 460, Loss: 0.001607, Val Loss: 0.001478\n",
      "Epoch 470, Loss: 0.002324, Val Loss: 0.002090\n",
      "------------- BEST MODEL - Epoch 477 - Val Loss 0.001470 -------------\n",
      "Epoch 480, Loss: 0.001642, Val Loss: 0.001555\n",
      "------------- BEST MODEL - Epoch 483 - Val Loss 0.001465 -------------\n",
      "------------- BEST MODEL - Epoch 486 - Val Loss 0.001408 -------------\n",
      "Epoch 490, Loss: 0.001744, Val Loss: 0.001612\n",
      "Epoch 500, Loss: 0.001799, Val Loss: 0.001663\n",
      "Epoch 510, Loss: 0.001683, Val Loss: 0.001510\n",
      "Epoch 520, Loss: 0.001363, Val Loss: 0.001451\n",
      "------------- BEST MODEL - Epoch 529 - Val Loss 0.001369 -------------\n",
      "Epoch 530, Loss: 0.001485, Val Loss: 0.001686\n",
      "Epoch 540, Loss: 0.001592, Val Loss: 0.001523\n",
      "Epoch 550, Loss: 0.001557, Val Loss: 0.001477\n",
      "Epoch 560, Loss: 0.001314, Val Loss: 0.001382\n",
      "------------- BEST MODEL - Epoch 570 - Val Loss 0.001352 -------------\n",
      "Epoch 570, Loss: 0.001288, Val Loss: 0.001352\n",
      "Epoch 580, Loss: 0.002125, Val Loss: 0.002085\n",
      "Epoch 590, Loss: 0.001330, Val Loss: 0.001514\n",
      "Epoch 600, Loss: 0.001643, Val Loss: 0.002007\n",
      "Epoch 610, Loss: 0.001164, Val Loss: 0.001369\n",
      "------------- BEST MODEL - Epoch 613 - Val Loss 0.001351 -------------\n",
      "Epoch 620, Loss: 0.001416, Val Loss: 0.001503\n",
      "------------- BEST MODEL - Epoch 623 - Val Loss 0.001327 -------------\n",
      "------------- BEST MODEL - Epoch 624 - Val Loss 0.001303 -------------\n",
      "Epoch 630, Loss: 0.001337, Val Loss: 0.001454\n",
      "Epoch 640, Loss: 0.001257, Val Loss: 0.001432\n",
      "------------- BEST MODEL - Epoch 645 - Val Loss 0.001269 -------------\n",
      "Epoch 650, Loss: 0.001246, Val Loss: 0.001440\n",
      "Epoch 660, Loss: 0.001124, Val Loss: 0.001301\n",
      "Epoch 670, Loss: 0.001585, Val Loss: 0.001522\n",
      "Epoch 680, Loss: 0.001238, Val Loss: 0.001365\n",
      "Epoch 690, Loss: 0.001515, Val Loss: 0.001507\n",
      "Epoch 700, Loss: 0.001209, Val Loss: 0.001449\n",
      "Epoch 710, Loss: 0.001101, Val Loss: 0.001573\n",
      "Epoch 720, Loss: 0.001017, Val Loss: 0.001282\n",
      "------------- BEST MODEL - Epoch 722 - Val Loss 0.001261 -------------\n",
      "------------- BEST MODEL - Epoch 728 - Val Loss 0.001252 -------------\n",
      "Epoch 730, Loss: 0.001159, Val Loss: 0.001321\n",
      "Epoch 740, Loss: 0.001287, Val Loss: 0.001517\n",
      "Epoch 750, Loss: 0.000866, Val Loss: 0.001346\n",
      "Epoch 760, Loss: 0.001076, Val Loss: 0.001370\n",
      "------------- BEST MODEL - Epoch 767 - Val Loss 0.001238 -------------\n",
      "Epoch 770, Loss: 0.001181, Val Loss: 0.001263\n",
      "Epoch 780, Loss: 0.001278, Val Loss: 0.001485\n",
      "Epoch 790, Loss: 0.001030, Val Loss: 0.001326\n",
      "Epoch 800, Loss: 0.001004, Val Loss: 0.001291\n",
      "Epoch 810, Loss: 0.001026, Val Loss: 0.001438\n",
      "------------- BEST MODEL - Epoch 817 - Val Loss 0.001232 -------------\n",
      "------------- BEST MODEL - Epoch 818 - Val Loss 0.001199 -------------\n",
      "Epoch 820, Loss: 0.000920, Val Loss: 0.001226\n",
      "Epoch 830, Loss: 0.001400, Val Loss: 0.001572\n",
      "Epoch 840, Loss: 0.000979, Val Loss: 0.001207\n",
      "Epoch 850, Loss: 0.001162, Val Loss: 0.001335\n",
      "------------- BEST MODEL - Epoch 853 - Val Loss 0.001191 -------------\n",
      "Epoch 860, Loss: 0.001020, Val Loss: 0.001294\n",
      "Epoch 870, Loss: 0.001074, Val Loss: 0.001300\n",
      "------------- BEST MODEL - Epoch 880 - Val Loss 0.001181 -------------\n",
      "Epoch 880, Loss: 0.000882, Val Loss: 0.001181\n",
      "Epoch 890, Loss: 0.001074, Val Loss: 0.001383\n",
      "Epoch 900, Loss: 0.001001, Val Loss: 0.001220\n",
      "Epoch 910, Loss: 0.001046, Val Loss: 0.001289\n",
      "------------- BEST MODEL - Epoch 914 - Val Loss 0.001173 -------------\n",
      "Epoch 920, Loss: 0.000988, Val Loss: 0.001239\n",
      "Epoch 930, Loss: 0.001029, Val Loss: 0.001316\n",
      "Epoch 940, Loss: 0.000943, Val Loss: 0.001301\n",
      "Epoch 950, Loss: 0.001515, Val Loss: 0.001508\n",
      "Epoch 960, Loss: 0.000836, Val Loss: 0.001174\n",
      "------------- BEST MODEL - Epoch 969 - Val Loss 0.001152 -------------\n",
      "Epoch 970, Loss: 0.000977, Val Loss: 0.001334\n",
      "------------- BEST MODEL - Epoch 973 - Val Loss 0.001144 -------------\n",
      "Epoch 980, Loss: 0.001187, Val Loss: 0.001376\n",
      "Epoch 990, Loss: 0.000994, Val Loss: 0.001165\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# add dataset here\n",
    "features = torch.tensor(data[:,:12], dtype=torch.float32)\n",
    "labels = torch.tensor(data[:,16:20], dtype=torch.float32)\n",
    "dataset = TensorDataset(features, labels)\n",
    "\n",
    "# dataloaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# model, loss, optimizer\n",
    "input_size = len(dataset[0][0])\n",
    "output_size = len(dataset[0][1])\n",
    "model = DeepNN(input_size=input_size, hidden_layers=[64,64], output_size=output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "writer = SummaryWriter()\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for X,Y in train_loader:\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X,Y in val_loader:\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './saved_models/best_model.pth')\n",
    "        print(f\"------------- BEST MODEL - Epoch {epoch} - Val Loss {val_loss:.6f} -------------\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462f6bba0c0d845",
   "metadata": {},
   "source": [
    "### 4. Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c53ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTwoBus:\n",
    "    def __init__(self, V_ext, P, Q, G, B, V_init, theta_init):\n",
    "        '''This class creates a simple 2-bus network.'''\n",
    "        self.V_ext = V_ext\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.G = G\n",
    "        self.B = B\n",
    "        self.V_init = V_init\n",
    "        self.theta_init = theta_init\n",
    "        self.net = pp.create_empty_network()\n",
    "        self.create_two_bus_grid()\n",
    "\n",
    "    def create_two_bus_grid(self):\n",
    "        # Create two buses with initialized voltage and angle\n",
    "        bus1 = pp.create_bus(self.net, vn_kv=20.0, name=\"Bus 1\")\n",
    "        bus2 = pp.create_bus(self.net, vn_kv=0.4, name=\"Bus 2\")\n",
    "    \n",
    "        # Initialize voltage and angle for buses\n",
    "        self.net.bus.loc[bus1, 'vm_pu'] = self.V_init[0]\n",
    "        self.net.bus.loc[bus1, 'va_degree'] = self.theta_init[0]\n",
    "        self.net.bus.loc[bus2, 'vm_pu'] = self.V_init[1]\n",
    "        self.net.bus.loc[bus2, 'va_degree'] = self.theta_init[1]\n",
    "    \n",
    "        # create a line between the two buses\n",
    "        pp.create_line_from_parameters(\n",
    "            self.net,\n",
    "            from_bus=0,\n",
    "            to_bus=1,\n",
    "            length_km=1.0,\n",
    "            r_ohm_per_km=1/self.G,\n",
    "            x_ohm_per_km=1/self.B,\n",
    "            c_nf_per_km=0.0,\n",
    "            g_us_per_km=0.0,\n",
    "            max_i_ka=100.0,\n",
    "        )\n",
    "\n",
    "        # Create a transformer between the two buses\n",
    "        # pp.create_transformer(self.net, bus1, bus2, std_type=\"0.25 MVA 20/0.4 kV\")\n",
    "    \n",
    "        # Create a load at bus 2 with specified P and Q\n",
    "        pp.create_load(self.net, bus2, p_mw=self.P, q_mvar=self.Q, name=\"Load\")\n",
    "    \n",
    "        # Create an external grid connection at bus 1 with specified G and B\n",
    "        pp.create_ext_grid(self.net, bus1, vm_pu=self.V_ext, name=\"Grid Connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec720f92675663a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HeydarianArdakaniA\\AppData\\Local\\Temp\\ipykernel_24340\\1919437371.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('./saved_models/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted voltage magnitudes: [1.0298587 1.0330721]\n",
      "Predicted voltage angles: [-0.00131004 -0.07823545]\n",
      "Reference voltage magnitudes: [0.92181296 0.92167052]\n",
      "Reference voltage angles: [ 0.         -0.10363299]\n",
      "Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "VM_PU_RANGE = [0.9, 1.1]  #\n",
    "P_MW_RANGE = [0.0, 0.2]  #\n",
    "Q_MVAR_RANGE = [0.0, 0.1]  #\n",
    "G_RANGE = [80, 120]  #\n",
    "B_RANGE = [0.01, 0.2]  #\n",
    "INIT_VM_PU_MIN = 0.9  #\n",
    "INIT_VM_PU_MAX = 1.1  #\n",
    "INIT_THETA_MAX = -1\n",
    "INIT_THETA_MIN = 1\n",
    "\n",
    "# net = pp.networks.example_simple()\n",
    "V_ext = np.random.uniform(VM_PU_RANGE[0], VM_PU_RANGE[1])\n",
    "P = np.random.uniform(P_MW_RANGE[0], P_MW_RANGE[1])\n",
    "Q = np.random.uniform(Q_MVAR_RANGE[0], Q_MVAR_RANGE[1])\n",
    "G = np.random.uniform(G_RANGE[0], G_RANGE[1])  # Short-circuit power in MVA\n",
    "B = np.random.uniform(B_RANGE[0], B_RANGE[1])  # Short-circuit impedance\n",
    "\n",
    "V_init = [\n",
    "    np.random.uniform(INIT_VM_PU_MIN, INIT_VM_PU_MAX),\n",
    "    np.random.uniform(INIT_VM_PU_MIN, INIT_VM_PU_MAX),\n",
    "]\n",
    "theta_init = [\n",
    "    np.random.uniform(INIT_THETA_MIN, INIT_THETA_MAX),\n",
    "    np.random.uniform(INIT_THETA_MIN, INIT_THETA_MAX),\n",
    "]\n",
    "\n",
    "Net = SimpleTwoBus(V_ext,P,Q,G,B,V_init,theta_init)\n",
    "net = Net.net\n",
    "pp.runpp(net, max_iteration=1, tolerance_mva=np.inf)\n",
    "\n",
    "# Prepare input\n",
    "Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "input_tensor = torch.FloatTensor(np.concatenate([\n",
    "    S.real, \n",
    "    S.imag,\n",
    "    Ybus.real.flatten(), \n",
    "    Ybus.imag.flatten(),\n",
    "]))\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "best_model = DeepNN(input_size=input_size, hidden_layers=[64,64], output_size=output_size)\n",
    "best_model.load_state_dict(torch.load('./saved_models/best_model.pth'))\n",
    "best_model.eval()\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Split prediction into voltage magnitudes and angles\n",
    "n_buses = len(net.bus)\n",
    "V_mag_pred = output[:n_buses].numpy()\n",
    "V_ang_pred = output[n_buses:].numpy()\n",
    "\n",
    "# Run power flow to ensure internal data is available\n",
    "# pp.runpp(net, calculate_voltage_angles=True)\n",
    "pp.runpp(net,\n",
    "         init=\"auto\",\n",
    "         init_vm_pu=V_mag_pred,\n",
    "         init_va_degree=V_ang_pred,\n",
    "         max_iteration=50,\n",
    "         tolerance_mva=1e-5)\n",
    "\n",
    "# Get reference values from the network\n",
    "V_mag_ref = net.res_bus.vm_pu.values\n",
    "V_ang_ref = net.res_bus.va_degree.values\n",
    "\n",
    "print(\"Predicted voltage magnitudes:\", V_mag_pred)\n",
    "print(\"Predicted voltage angles:\", V_ang_pred)\n",
    "print(\"Reference voltage magnitudes:\", V_mag_ref)\n",
    "print(\"Reference voltage angles:\", V_ang_ref)\n",
    "print(\"Iterations:\", net._ppc[\"iterations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750e487a27f2cbf",
   "metadata": {},
   "source": [
    "### 6. Additional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2045875b3bbdec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, val_loader, criterion):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             batch_inputs = batch['input']\n",
    "#             batch_targets = batch['output']\n",
    "#             outputs = model(batch_inputs)\n",
    "#             loss = criterion(outputs, batch_targets)\n",
    "#             val_loss += loss.item()\n",
    "#     val_loss /= len(val_loader)\n",
    "#     return val_loss\n",
    "\n",
    "# def hyperparameter_tuning(base_network, param_grid):\n",
    "#     best_model = None\n",
    "#     best_loss = float('inf')\n",
    "#     dataset = PowerFlowDataset(base_network)\n",
    "#     train_size = int(0.8 * len(dataset))\n",
    "#     val_size = len(dataset) - train_size\n",
    "#     _, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#     for params in ParameterGrid(param_grid):\n",
    "#         print(f\"Training with parameters: {params}\")\n",
    "#         model = train_power_flow_model(base_network, num_epochs=params['num_epochs'], batch_size=params['batch_size'])\n",
    "#         val_loss = evaluate_model(model, val_loader, nn.MSELoss())\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model\n",
    "#     return best_model\n",
    "\n",
    "# base_network = pp.networks.example_simple()\n",
    "# param_grid = {\n",
    "#     'num_epochs': [50, 100],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'hidden_size': [256, 512]\n",
    "# }\n",
    "# best_model = hyperparameter_tuning(base_network, param_grid)\n",
    "# # ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
