{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Flow Neural Network Training with Pandapower\n",
    "\n",
    "This notebook demonstrates how to create a pipeline for training a neural network to learn power flow solutions using pandapower's internal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter # for pytorch visualization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # normalize input features and target values\n",
    "from sklearn.model_selection import ParameterGrid # for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Dataset Class\n",
    "\n",
    "First, we create a PyTorch dataset class that interfaces with pandapower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerFlowDataset(Dataset):\n",
    "    def __init__(self, base_network, num_samples=1000):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with a base network and number of samples.\n",
    "        \n",
    "        Parameters:\n",
    "        base_network (pandapowerNet): The base pandapower network.\n",
    "        num_samples (int): Number of samples to generate.\n",
    "        \"\"\"\n",
    "        self.base_net = base_network\n",
    "        self.num_samples = num_samples\n",
    "        self.samples = []\n",
    "        self.scaler_input = StandardScaler()\n",
    "        self.scaler_output = StandardScaler()\n",
    "        self.generate_samples()\n",
    "        self.normalize_samples()\n",
    "        \n",
    "    def generate_samples(self):\n",
    "        \"\"\"\n",
    "        Generate samples by randomly modifying the base network and running power flow.\n",
    "        \"\"\"\n",
    "        for _ in range(self.num_samples):\n",
    "            net = self.base_net.deepcopy()\n",
    "            # Modify the network randomly to generate different samples\n",
    "            # For example, change load values, generator outputs, etc.\n",
    "            pp.runpp(net)\n",
    "            Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "            S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "            V_mag = net.res_bus.vm_pu.values\n",
    "            V_ang = net.res_bus.va_degree.values\n",
    "            self.samples.append({\n",
    "                \"input\": np.concatenate([Ybus.real.flatten(), \n",
    "                                         Ybus.imag.flatten(),\n",
    "                                         S.real, \n",
    "                                         S.imag]),\n",
    "                \"output\": np.concatenate([V_mag, V_ang])\n",
    "            })\n",
    "\n",
    "    \n",
    "    def normalize_samples(self):\n",
    "        \"\"\"\n",
    "        Normalize the input features and target values.\n",
    "        \"\"\"\n",
    "        inputs = np.array([sample[\"input\"] for sample in self.samples])\n",
    "        outputs = np.array([sample[\"output\"] for sample in self.samples])\n",
    "        self.scaler_input.fit(inputs)\n",
    "        self.scaler_output.fit(outputs)\n",
    "        for sample in self.samples:\n",
    "            sample[\"input\"] = self.scaler_input.transform([sample[\"input\"]])[0]\n",
    "            sample[\"output\"] = self.scaler_output.transform([sample[\"output\"]])[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return {\n",
    "            'input': torch.FloatTensor(sample['input']),\n",
    "            'output': torch.FloatTensor(sample['output'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Neural Network Model\n",
    "\n",
    "Next, we define our neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerFlowDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PowerFlowDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline\n",
    "\n",
    "Create the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_power_flow_model(base_network, num_epochs=100, batch_size=32):\n",
    "    # Create dataset\n",
    "    dataset = PowerFlowDataset(base_network)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = len(dataset[0]['input'])\n",
    "    output_size = len(dataset[0]['output'])\n",
    "    model = PowerFlowDNN(input_size=input_size, hidden_size=512, output_size=output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch_inputs = batch['input']\n",
    "            batch_targets = batch['output']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_inputs = batch['input']\n",
    "                batch_targets = batch['output']\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Log training and validation loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usage Example\n",
    "\n",
    "Here's how to use the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_power_flow(model, net):\n",
    "    # Run power flow to ensure internal data is available\n",
    "    pp.runpp(net, calculate_voltage_angles=True)\n",
    "    \n",
    "    # Prepare input\n",
    "    Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "    S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "    input_tensor = torch.FloatTensor(np.concatenate([\n",
    "        Ybus.real.flatten(), \n",
    "        Ybus.imag.flatten(),\n",
    "        S.real, \n",
    "        S.imag\n",
    "    ]))\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    # Split prediction into voltage magnitudes and angles\n",
    "    n_buses = len(net.bus)\n",
    "    V_mag_pred = output[:n_buses].numpy()\n",
    "    V_ang_pred = output[n_buses:].numpy()\n",
    "    \n",
    "    # Get reference values from the network\n",
    "    V_mag_ref = net.res_bus.vm_pu.values\n",
    "    V_ang_ref = net.res_bus.va_degree.values\n",
    "    \n",
    "    return V_mag_pred, V_ang_pred, V_mag_ref, V_ang_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "Here's how to put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test network\n",
    "# net = pp.create_empty_network()\n",
    "# Add your network elements here...\n",
    "net = pp.networks.example_simple()\n",
    "\n",
    "# Train the model\n",
    "model = train_power_flow_model(net)\n",
    "\n",
    "# Make predictions\n",
    "V_mag_pred, V_ang_pred, V_mag_ref, V_ang_ref = predict_power_flow(model, net)\n",
    "print(\"Predicted voltage magnitudes:\", V_mag_pred)\n",
    "print(\"Predicted voltage angles:\", V_ang_pred)\n",
    "print(\"Reference voltage magnitudes:\", V_mag_ref)\n",
    "print(\"Reference voltage angles:\", V_ang_ref)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pp.plotting.simple_plot(net)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_net=pp.networks.example_simple()\n",
    "num_samples=1\n",
    "samples=[]\n",
    "for _ in range(num_samples):\n",
    "        net = base_net.deepcopy()\n",
    "        # Modify the network randomly to generate different samples\n",
    "        # For example, change load values, generator outputs, etc.\n",
    "        v_init = np.array(np.random.uniform(low=0.9,high=1.1,size=(7,)))  # your voltage magnitude initialization\n",
    "        theta_init = np.array(np.random.uniform(low=-20,high=20,size=(7,)))  # your voltage angle initialization in degrees\n",
    "        # Run power flow with initialization from our initial states\n",
    "        pp.runpp(net, init = \"auto\", init_vm_pu=v_init, init_va_degree=theta_init) # x < 10\n",
    "        Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "        S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "        V_mag = net.res_bus.vm_pu.values\n",
    "        V_ang = net.res_bus.va_degree.values\n",
    "        samples.append({\n",
    "            \"input\": np.concatenate([Ybus.real.flatten(),\n",
    "                                     Ybus.imag.flatten(),\n",
    "                                     S.real,\n",
    "                                     S.imag]),\n",
    "            \"output\": np.concatenate([V_mag, V_ang])\n",
    "        })\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_inputs = batch['input']\n",
    "            batch_targets = batch['output']\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "def hyperparameter_tuning(base_network, param_grid):\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    dataset = PowerFlowDataset(base_network)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    _, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Training with parameters: {params}\")\n",
    "        model = train_power_flow_model(base_network, num_epochs=params['num_epochs'], batch_size=params['batch_size'])\n",
    "        val_loss = evaluate_model(model, val_loader, nn.MSELoss())\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "    return best_model\n",
    "\n",
    "base_network = pp.networks.example_simple()\n",
    "param_grid = {\n",
    "    'num_epochs': [50, 100],\n",
    "    'batch_size': [16, 32],\n",
    "    'hidden_size': [256, 512]\n",
    "}\n",
    "best_model = hyperparameter_tuning(base_network, param_grid)\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ict25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
