{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac13d970-99d2-4746-a315-98d6090beece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandapower as pp\n",
    "import numpy as np\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter # for pytorch visualization\n",
    " \n",
    "from sklearn.preprocessing import StandardScaler # normalize input features and target values\n",
    "from sklearn.model_selection import ParameterGrid # for hyperparameter tuning\n",
    " \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data = np.load('./vector_data_solution_prediction.npy')\n",
    "print(data.shape)\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(DeepNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Training setup\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    " \n",
    "# add dataset here\n",
    "features = torch.tensor(data[:,:12], dtype=torch.float32)\n",
    "labels = torch.tensor(data[:,16:20], dtype=torch.float32)\n",
    "dataset = TensorDataset(features, labels)\n",
    " \n",
    "# dataloaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    " \n",
    "# model, loss, optimizer\n",
    "input_size = len(dataset[0][0])\n",
    "output_size = len(dataset[0][1])\n",
    "model = DeepNN(input_size=input_size, hidden_layers=[64,64], output_size=output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    " \n",
    "# training loop\n",
    "writer = SummaryWriter()\n",
    "best_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a817e211-8b24-4874-a296-3367ad825e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- BEST MODEL - Epoch 0 - Val Loss 0.200048 -------------\n",
      "Epoch 000, Loss: 0.212298, Val Loss: 0.200048\n",
      "------------- BEST MODEL - Epoch 1 - Val Loss 0.127928 -------------\n",
      "------------- BEST MODEL - Epoch 2 - Val Loss 0.110318 -------------\n",
      "------------- BEST MODEL - Epoch 3 - Val Loss 0.101578 -------------\n",
      "------------- BEST MODEL - Epoch 4 - Val Loss 0.095117 -------------\n",
      "------------- BEST MODEL - Epoch 5 - Val Loss 0.086722 -------------\n",
      "------------- BEST MODEL - Epoch 6 - Val Loss 0.079738 -------------\n",
      "------------- BEST MODEL - Epoch 7 - Val Loss 0.073011 -------------\n",
      "------------- BEST MODEL - Epoch 8 - Val Loss 0.067263 -------------\n",
      "------------- BEST MODEL - Epoch 9 - Val Loss 0.062007 -------------\n",
      "------------- BEST MODEL - Epoch 10 - Val Loss 0.057230 -------------\n",
      "Epoch 010, Loss: 0.066107, Val Loss: 0.057230\n",
      "------------- BEST MODEL - Epoch 11 - Val Loss 0.051391 -------------\n",
      "------------- BEST MODEL - Epoch 12 - Val Loss 0.047160 -------------\n",
      "------------- BEST MODEL - Epoch 13 - Val Loss 0.042126 -------------\n",
      "------------- BEST MODEL - Epoch 14 - Val Loss 0.037945 -------------\n",
      "------------- BEST MODEL - Epoch 15 - Val Loss 0.035243 -------------\n",
      "------------- BEST MODEL - Epoch 16 - Val Loss 0.031556 -------------\n",
      "------------- BEST MODEL - Epoch 17 - Val Loss 0.029107 -------------\n",
      "------------- BEST MODEL - Epoch 18 - Val Loss 0.026464 -------------\n",
      "------------- BEST MODEL - Epoch 19 - Val Loss 0.023870 -------------\n",
      "------------- BEST MODEL - Epoch 20 - Val Loss 0.021386 -------------\n",
      "Epoch 020, Loss: 0.023166, Val Loss: 0.021386\n",
      "------------- BEST MODEL - Epoch 21 - Val Loss 0.019172 -------------\n",
      "------------- BEST MODEL - Epoch 22 - Val Loss 0.018061 -------------\n",
      "------------- BEST MODEL - Epoch 23 - Val Loss 0.016370 -------------\n",
      "------------- BEST MODEL - Epoch 24 - Val Loss 0.015232 -------------\n",
      "------------- BEST MODEL - Epoch 26 - Val Loss 0.014395 -------------\n",
      "------------- BEST MODEL - Epoch 27 - Val Loss 0.013002 -------------\n",
      "------------- BEST MODEL - Epoch 28 - Val Loss 0.012745 -------------\n",
      "------------- BEST MODEL - Epoch 29 - Val Loss 0.011931 -------------\n",
      "------------- BEST MODEL - Epoch 30 - Val Loss 0.011728 -------------\n",
      "Epoch 030, Loss: 0.011763, Val Loss: 0.011728\n",
      "------------- BEST MODEL - Epoch 31 - Val Loss 0.011386 -------------\n",
      "------------- BEST MODEL - Epoch 32 - Val Loss 0.010895 -------------\n",
      "------------- BEST MODEL - Epoch 34 - Val Loss 0.010786 -------------\n",
      "------------- BEST MODEL - Epoch 35 - Val Loss 0.010604 -------------\n",
      "------------- BEST MODEL - Epoch 36 - Val Loss 0.010472 -------------\n",
      "Epoch 040, Loss: 0.010203, Val Loss: 0.010578\n",
      "------------- BEST MODEL - Epoch 41 - Val Loss 0.010117 -------------\n",
      "------------- BEST MODEL - Epoch 45 - Val Loss 0.009878 -------------\n",
      "------------- BEST MODEL - Epoch 48 - Val Loss 0.009712 -------------\n",
      "------------- BEST MODEL - Epoch 49 - Val Loss 0.009616 -------------\n",
      "------------- BEST MODEL - Epoch 50 - Val Loss 0.009551 -------------\n",
      "Epoch 050, Loss: 0.009361, Val Loss: 0.009551\n",
      "------------- BEST MODEL - Epoch 54 - Val Loss 0.009513 -------------\n",
      "------------- BEST MODEL - Epoch 56 - Val Loss 0.009382 -------------\n",
      "Epoch 060, Loss: 0.009828, Val Loss: 0.009807\n",
      "------------- BEST MODEL - Epoch 64 - Val Loss 0.009232 -------------\n",
      "------------- BEST MODEL - Epoch 68 - Val Loss 0.009058 -------------\n",
      "Epoch 070, Loss: 0.009322, Val Loss: 0.009408\n",
      "------------- BEST MODEL - Epoch 74 - Val Loss 0.008914 -------------\n",
      "------------- BEST MODEL - Epoch 76 - Val Loss 0.008869 -------------\n",
      "------------- BEST MODEL - Epoch 80 - Val Loss 0.008827 -------------\n",
      "Epoch 080, Loss: 0.008512, Val Loss: 0.008827\n",
      "------------- BEST MODEL - Epoch 82 - Val Loss 0.008564 -------------\n",
      "Epoch 090, Loss: 0.009141, Val Loss: 0.009012\n",
      "------------- BEST MODEL - Epoch 92 - Val Loss 0.008325 -------------\n",
      "------------- BEST MODEL - Epoch 95 - Val Loss 0.008130 -------------\n",
      "------------- BEST MODEL - Epoch 96 - Val Loss 0.008034 -------------\n",
      "Epoch 100, Loss: 0.007775, Val Loss: 0.008139\n",
      "------------- BEST MODEL - Epoch 101 - Val Loss 0.007988 -------------\n",
      "------------- BEST MODEL - Epoch 102 - Val Loss 0.007649 -------------\n",
      "------------- BEST MODEL - Epoch 105 - Val Loss 0.007538 -------------\n",
      "------------- BEST MODEL - Epoch 109 - Val Loss 0.007523 -------------\n",
      "------------- BEST MODEL - Epoch 110 - Val Loss 0.007508 -------------\n",
      "Epoch 110, Loss: 0.007397, Val Loss: 0.007508\n",
      "------------- BEST MODEL - Epoch 111 - Val Loss 0.007430 -------------\n",
      "------------- BEST MODEL - Epoch 115 - Val Loss 0.007026 -------------\n",
      "Epoch 120, Loss: 0.007275, Val Loss: 0.007393\n",
      "------------- BEST MODEL - Epoch 121 - Val Loss 0.006972 -------------\n",
      "------------- BEST MODEL - Epoch 123 - Val Loss 0.006578 -------------\n",
      "------------- BEST MODEL - Epoch 124 - Val Loss 0.006439 -------------\n",
      "Epoch 130, Loss: 0.006430, Val Loss: 0.007192\n",
      "------------- BEST MODEL - Epoch 137 - Val Loss 0.006000 -------------\n",
      "------------- BEST MODEL - Epoch 138 - Val Loss 0.005784 -------------\n",
      "Epoch 140, Loss: 0.007819, Val Loss: 0.007616\n",
      "------------- BEST MODEL - Epoch 141 - Val Loss 0.005751 -------------\n",
      "------------- BEST MODEL - Epoch 144 - Val Loss 0.005566 -------------\n",
      "------------- BEST MODEL - Epoch 145 - Val Loss 0.005525 -------------\n",
      "------------- BEST MODEL - Epoch 146 - Val Loss 0.005325 -------------\n",
      "------------- BEST MODEL - Epoch 148 - Val Loss 0.005195 -------------\n",
      "Epoch 150, Loss: 0.005315, Val Loss: 0.005453\n",
      "------------- BEST MODEL - Epoch 153 - Val Loss 0.005147 -------------\n",
      "------------- BEST MODEL - Epoch 155 - Val Loss 0.005043 -------------\n",
      "------------- BEST MODEL - Epoch 160 - Val Loss 0.004956 -------------\n",
      "Epoch 160, Loss: 0.004671, Val Loss: 0.004956\n",
      "------------- BEST MODEL - Epoch 162 - Val Loss 0.004929 -------------\n",
      "Epoch 170, Loss: 0.007155, Val Loss: 0.006325\n",
      "------------- BEST MODEL - Epoch 172 - Val Loss 0.004816 -------------\n",
      "------------- BEST MODEL - Epoch 179 - Val Loss 0.004474 -------------\n",
      "Epoch 180, Loss: 0.004128, Val Loss: 0.004579\n",
      "Epoch 190, Loss: 0.004863, Val Loss: 0.004856\n",
      "------------- BEST MODEL - Epoch 196 - Val Loss 0.004317 -------------\n",
      "------------- BEST MODEL - Epoch 198 - Val Loss 0.004220 -------------\n",
      "Epoch 200, Loss: 0.005384, Val Loss: 0.004915\n",
      "------------- BEST MODEL - Epoch 207 - Val Loss 0.004208 -------------\n",
      "Epoch 210, Loss: 0.006538, Val Loss: 0.005720\n",
      "------------- BEST MODEL - Epoch 212 - Val Loss 0.004038 -------------\n",
      "------------- BEST MODEL - Epoch 214 - Val Loss 0.003674 -------------\n",
      "------------- BEST MODEL - Epoch 216 - Val Loss 0.003456 -------------\n",
      "------------- BEST MODEL - Epoch 218 - Val Loss 0.003201 -------------\n",
      "Epoch 220, Loss: 0.004218, Val Loss: 0.003256\n",
      "------------- BEST MODEL - Epoch 221 - Val Loss 0.003181 -------------\n",
      "------------- BEST MODEL - Epoch 224 - Val Loss 0.002649 -------------\n",
      "------------- BEST MODEL - Epoch 228 - Val Loss 0.002581 -------------\n",
      "Epoch 230, Loss: 0.004027, Val Loss: 0.003104\n",
      "------------- BEST MODEL - Epoch 231 - Val Loss 0.002224 -------------\n",
      "------------- BEST MODEL - Epoch 234 - Val Loss 0.002128 -------------\n",
      "Epoch 240, Loss: 0.002427, Val Loss: 0.002908\n",
      "------------- BEST MODEL - Epoch 241 - Val Loss 0.002115 -------------\n",
      "------------- BEST MODEL - Epoch 249 - Val Loss 0.001921 -------------\n",
      "Epoch 250, Loss: 0.002883, Val Loss: 0.003106\n",
      "Epoch 260, Loss: 0.002213, Val Loss: 0.002168\n",
      "Epoch 270, Loss: 0.001782, Val Loss: 0.001963\n",
      "Epoch 280, Loss: 0.002028, Val Loss: 0.002149\n",
      "------------- BEST MODEL - Epoch 285 - Val Loss 0.001881 -------------\n",
      "Epoch 290, Loss: 0.001537, Val Loss: 0.002107\n",
      "------------- BEST MODEL - Epoch 298 - Val Loss 0.001847 -------------\n",
      "Epoch 300, Loss: 0.001800, Val Loss: 0.001951\n",
      "------------- BEST MODEL - Epoch 306 - Val Loss 0.001844 -------------\n",
      "------------- BEST MODEL - Epoch 308 - Val Loss 0.001843 -------------\n",
      "Epoch 310, Loss: 0.002512, Val Loss: 0.002547\n",
      "Epoch 320, Loss: 0.001639, Val Loss: 0.002087\n",
      "------------- BEST MODEL - Epoch 330 - Val Loss 0.001840 -------------\n",
      "Epoch 330, Loss: 0.001621, Val Loss: 0.001840\n",
      "Epoch 340, Loss: 0.001360, Val Loss: 0.001931\n",
      "------------- BEST MODEL - Epoch 344 - Val Loss 0.001726 -------------\n",
      "Epoch 350, Loss: 0.001420, Val Loss: 0.002196\n",
      "Epoch 360, Loss: 0.001934, Val Loss: 0.001986\n",
      "Epoch 370, Loss: 0.004309, Val Loss: 0.005015\n",
      "Epoch 380, Loss: 0.001884, Val Loss: 0.001949\n",
      "Epoch 390, Loss: 0.002026, Val Loss: 0.002190\n",
      "Epoch 400, Loss: 0.001669, Val Loss: 0.002052\n",
      "Epoch 410, Loss: 0.001745, Val Loss: 0.002211\n",
      "------------- BEST MODEL - Epoch 416 - Val Loss 0.001684 -------------\n",
      "Epoch 420, Loss: 0.001760, Val Loss: 0.002037\n",
      "Epoch 430, Loss: 0.001221, Val Loss: 0.001963\n",
      "------------- BEST MODEL - Epoch 434 - Val Loss 0.001566 -------------\n",
      "Epoch 440, Loss: 0.001746, Val Loss: 0.001906\n",
      "------------- BEST MODEL - Epoch 443 - Val Loss 0.001553 -------------\n",
      "Epoch 450, Loss: 0.001392, Val Loss: 0.001843\n",
      "Epoch 460, Loss: 0.001127, Val Loss: 0.001733\n",
      "------------- BEST MODEL - Epoch 464 - Val Loss 0.001515 -------------\n",
      "------------- BEST MODEL - Epoch 468 - Val Loss 0.001494 -------------\n",
      "Epoch 470, Loss: 0.001074, Val Loss: 0.001702\n",
      "Epoch 480, Loss: 0.001000, Val Loss: 0.001582\n",
      "Epoch 490, Loss: 0.001162, Val Loss: 0.001579\n",
      "------------- BEST MODEL - Epoch 496 - Val Loss 0.001448 -------------\n",
      "Epoch 500, Loss: 0.001407, Val Loss: 0.001983\n",
      "Epoch 510, Loss: 0.001121, Val Loss: 0.001515\n",
      "------------- BEST MODEL - Epoch 513 - Val Loss 0.001439 -------------\n",
      "Epoch 520, Loss: 0.001231, Val Loss: 0.001506\n",
      "------------- BEST MODEL - Epoch 521 - Val Loss 0.001427 -------------\n",
      "------------- BEST MODEL - Epoch 525 - Val Loss 0.001413 -------------\n",
      "------------- BEST MODEL - Epoch 526 - Val Loss 0.001370 -------------\n",
      "Epoch 530, Loss: 0.000797, Val Loss: 0.001419\n",
      "------------- BEST MODEL - Epoch 531 - Val Loss 0.001340 -------------\n",
      "Epoch 540, Loss: 0.000804, Val Loss: 0.001364\n",
      "------------- BEST MODEL - Epoch 545 - Val Loss 0.001331 -------------\n",
      "------------- BEST MODEL - Epoch 550 - Val Loss 0.001298 -------------\n",
      "Epoch 550, Loss: 0.000864, Val Loss: 0.001298\n",
      "Epoch 560, Loss: 0.000810, Val Loss: 0.001416\n",
      "------------- BEST MODEL - Epoch 567 - Val Loss 0.001298 -------------\n",
      "Epoch 570, Loss: 0.000978, Val Loss: 0.001482\n",
      "Epoch 580, Loss: 0.000832, Val Loss: 0.001396\n",
      "------------- BEST MODEL - Epoch 581 - Val Loss 0.001251 -------------\n",
      "Epoch 590, Loss: 0.000802, Val Loss: 0.001347\n",
      "------------- BEST MODEL - Epoch 592 - Val Loss 0.001235 -------------\n",
      "------------- BEST MODEL - Epoch 593 - Val Loss 0.001226 -------------\n",
      "Epoch 600, Loss: 0.000753, Val Loss: 0.001271\n",
      "------------- BEST MODEL - Epoch 608 - Val Loss 0.001214 -------------\n",
      "Epoch 610, Loss: 0.001015, Val Loss: 0.001348\n",
      "Epoch 620, Loss: 0.000765, Val Loss: 0.001218\n",
      "------------- BEST MODEL - Epoch 628 - Val Loss 0.001200 -------------\n",
      "Epoch 630, Loss: 0.001053, Val Loss: 0.001530\n",
      "Epoch 640, Loss: 0.000751, Val Loss: 0.001265\n",
      "------------- BEST MODEL - Epoch 645 - Val Loss 0.001196 -------------\n",
      "Epoch 650, Loss: 0.000872, Val Loss: 0.001268\n",
      "------------- BEST MODEL - Epoch 655 - Val Loss 0.001189 -------------\n",
      "Epoch 660, Loss: 0.000880, Val Loss: 0.001283\n",
      "------------- BEST MODEL - Epoch 667 - Val Loss 0.001179 -------------\n",
      "------------- BEST MODEL - Epoch 669 - Val Loss 0.001156 -------------\n",
      "Epoch 670, Loss: 0.000759, Val Loss: 0.001243\n",
      "Epoch 680, Loss: 0.000957, Val Loss: 0.001319\n",
      "------------- BEST MODEL - Epoch 683 - Val Loss 0.001148 -------------\n",
      "Epoch 690, Loss: 0.000905, Val Loss: 0.001215\n",
      "------------- BEST MODEL - Epoch 699 - Val Loss 0.001142 -------------\n",
      "Epoch 700, Loss: 0.000951, Val Loss: 0.001285\n",
      "Epoch 710, Loss: 0.000958, Val Loss: 0.001274\n",
      "Epoch 720, Loss: 0.000840, Val Loss: 0.001260\n",
      "------------- BEST MODEL - Epoch 730 - Val Loss 0.001129 -------------\n",
      "Epoch 730, Loss: 0.000777, Val Loss: 0.001129\n",
      "Epoch 740, Loss: 0.000899, Val Loss: 0.001208\n",
      "Epoch 750, Loss: 0.000835, Val Loss: 0.001190\n",
      "Epoch 760, Loss: 0.000828, Val Loss: 0.001170\n",
      "------------- BEST MODEL - Epoch 761 - Val Loss 0.001116 -------------\n",
      "Epoch 770, Loss: 0.000818, Val Loss: 0.001161\n",
      "Epoch 780, Loss: 0.000887, Val Loss: 0.001190\n",
      "------------- BEST MODEL - Epoch 784 - Val Loss 0.001109 -------------\n",
      "Epoch 790, Loss: 0.001010, Val Loss: 0.001284\n",
      "------------- BEST MODEL - Epoch 796 - Val Loss 0.001107 -------------\n",
      "Epoch 800, Loss: 0.000937, Val Loss: 0.001265\n",
      "------------- BEST MODEL - Epoch 810 - Val Loss 0.001098 -------------\n",
      "Epoch 810, Loss: 0.000712, Val Loss: 0.001098\n",
      "------------- BEST MODEL - Epoch 813 - Val Loss 0.001079 -------------\n",
      "Epoch 820, Loss: 0.000744, Val Loss: 0.001123\n",
      "Epoch 830, Loss: 0.000840, Val Loss: 0.001134\n",
      "------------- BEST MODEL - Epoch 833 - Val Loss 0.001077 -------------\n",
      "Epoch 840, Loss: 0.000804, Val Loss: 0.001169\n",
      "Epoch 850, Loss: 0.000991, Val Loss: 0.001184\n",
      "------------- BEST MODEL - Epoch 855 - Val Loss 0.001063 -------------\n",
      "------------- BEST MODEL - Epoch 857 - Val Loss 0.001036 -------------\n",
      "Epoch 860, Loss: 0.000809, Val Loss: 0.001085\n",
      "Epoch 870, Loss: 0.001001, Val Loss: 0.001215\n",
      "Epoch 880, Loss: 0.000985, Val Loss: 0.001148\n",
      "------------- BEST MODEL - Epoch 886 - Val Loss 0.001001 -------------\n",
      "Epoch 890, Loss: 0.000775, Val Loss: 0.001031\n",
      "Epoch 900, Loss: 0.000935, Val Loss: 0.001034\n",
      "Epoch 910, Loss: 0.000825, Val Loss: 0.001125\n",
      "Epoch 920, Loss: 0.000561, Val Loss: 0.001161\n",
      "------------- BEST MODEL - Epoch 928 - Val Loss 0.000997 -------------\n",
      "Epoch 930, Loss: 0.000899, Val Loss: 0.001162\n",
      "------------- BEST MODEL - Epoch 934 - Val Loss 0.000974 -------------\n",
      "Epoch 940, Loss: 0.001128, Val Loss: 0.001222\n",
      "Epoch 950, Loss: 0.000864, Val Loss: 0.001014\n",
      "Epoch 960, Loss: 0.000967, Val Loss: 0.001047\n",
      "------------- BEST MODEL - Epoch 969 - Val Loss 0.000973 -------------\n",
      "Epoch 970, Loss: 0.000856, Val Loss: 0.001040\n",
      "Epoch 980, Loss: 0.000910, Val Loss: 0.001025\n",
      "Epoch 990, Loss: 0.000662, Val Loss: 0.000985\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for X,Y in train_loader:\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X,Y in val_loader:\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    " \n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    " \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './best_model.pth')\n",
    "        print(f\"------------- BEST MODEL - Epoch {epoch} - Val Loss {val_loss:.6f} -------------\")\n",
    " \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}\")\n",
    " \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c2049d-6011-4087-a26b-0d89fe912242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Raphson function called!!\n",
      "Newton-Raphson function enter 2\n",
      "Newton-Raphson function enter 4\n",
      "enter 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86139\\AppData\\Local\\Temp\\ipykernel_23936\\3001867025.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('./best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Raphson function called!!\n",
      "Newton-Raphson function enter 4\n",
      "enter 3\n",
      "Predicted voltage magnitudes: [1.0279542 1.0293843]\n",
      "Predicted voltage angles: [-0.00223596 -0.01798306]\n",
      "Reference voltage magnitudes: [1.03784063 1.03708116]\n",
      "Reference voltage angles: [ 0.         -0.04152407]\n",
      "Iterations: 2\n"
     ]
    }
   ],
   "source": [
    "class SimpleTwoBus:\n",
    "    def __init__(self, V_ext, P, Q, G, B, V_init, theta_init):\n",
    "        '''This class creates a simple 2-bus network.'''\n",
    "        self.V_ext = V_ext\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.G = G\n",
    "        self.B = B\n",
    "        self.V_init = V_init\n",
    "        self.theta_init = theta_init\n",
    "        self.net = pp.create_empty_network()\n",
    "        self.create_two_bus_grid()\n",
    " \n",
    "    def create_two_bus_grid(self):\n",
    "        # Create two buses with initialized voltage and angle\n",
    "        bus1 = pp.create_bus(self.net, vn_kv=20.0, name=\"Bus 1\")\n",
    "        bus2 = pp.create_bus(self.net, vn_kv=0.4, name=\"Bus 2\")\n",
    "   \n",
    "        # Initialize voltage and angle for buses\n",
    "        self.net.bus.loc[bus1, 'vm_pu'] = self.V_init[0]\n",
    "        self.net.bus.loc[bus1, 'va_degree'] = self.theta_init[0]\n",
    "        self.net.bus.loc[bus2, 'vm_pu'] = self.V_init[1]\n",
    "        self.net.bus.loc[bus2, 'va_degree'] = self.theta_init[1]\n",
    "   \n",
    "        # create a line between the two buses\n",
    "        pp.create_line_from_parameters(\n",
    "            self.net,\n",
    "            from_bus=0,\n",
    "            to_bus=1,\n",
    "            length_km=1.0,\n",
    "            r_ohm_per_km=1/self.G,\n",
    "            x_ohm_per_km=1/self.B,\n",
    "            c_nf_per_km=0.0,\n",
    "            g_us_per_km=0.0,\n",
    "            max_i_ka=100.0,\n",
    "        )\n",
    " \n",
    "        # Create a transformer between the two buses\n",
    "        # pp.create_transformer(self.net, bus1, bus2, std_type=\"0.25 MVA 20/0.4 kV\")\n",
    "   \n",
    "        # Create a load at bus 2 with specified P and Q\n",
    "        pp.create_load(self.net, bus2, p_mw=self.P, q_mvar=self.Q, name=\"Load\")\n",
    "   \n",
    "        # Create an external grid connection at bus 1 with specified G and B\n",
    "        pp.create_ext_grid(self.net, bus1, vm_pu=self.V_ext, name=\"Grid Connection\")\n",
    "        \n",
    "VM_PU_RANGE = [0.9, 1.1]  #\n",
    "P_MW_RANGE = [0.0, 0.2]  #\n",
    "Q_MVAR_RANGE = [0.0, 0.1]  #\n",
    "G_RANGE = [80, 120]  #\n",
    "B_RANGE = [0.01, 0.2]  #\n",
    "INIT_VM_PU_MIN = 0.9  #\n",
    "INIT_VM_PU_MAX = 1.1  #\n",
    "INIT_THETA_MAX = -1\n",
    "INIT_THETA_MIN = 1\n",
    " \n",
    "# net = pp.networks.example_simple()\n",
    "V_ext = np.random.uniform(VM_PU_RANGE[0], VM_PU_RANGE[1])\n",
    "P = np.random.uniform(P_MW_RANGE[0], P_MW_RANGE[1])\n",
    "Q = np.random.uniform(Q_MVAR_RANGE[0], Q_MVAR_RANGE[1])\n",
    "G = np.random.uniform(G_RANGE[0], G_RANGE[1])  # Short-circuit power in MVA\n",
    "B = np.random.uniform(B_RANGE[0], B_RANGE[1])  # Short-circuit impedance\n",
    " \n",
    "V_init = [\n",
    "    np.random.uniform(INIT_VM_PU_MIN, INIT_VM_PU_MAX),\n",
    "    np.random.uniform(INIT_VM_PU_MIN, INIT_VM_PU_MAX),\n",
    "]\n",
    "theta_init = [\n",
    "    np.random.uniform(INIT_THETA_MIN, INIT_THETA_MAX),\n",
    "    np.random.uniform(INIT_THETA_MIN, INIT_THETA_MAX),\n",
    "]\n",
    " \n",
    "Net = SimpleTwoBus(V_ext,P,Q,G,B,V_init,theta_init)\n",
    "net = Net.net\n",
    "pp.runpp(net, max_iteration=1, tolerance_mva=np.inf)\n",
    " \n",
    "# Prepare input\n",
    "Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "input_tensor = torch.FloatTensor(np.concatenate([\n",
    "    S.real,\n",
    "    S.imag,\n",
    "    Ybus.real.flatten(),\n",
    "    Ybus.imag.flatten(),\n",
    "]))\n",
    " \n",
    "# Load the best model\n",
    "best_model = DeepNN(input_size=input_size, hidden_layers=[64,64], output_size=output_size)\n",
    "best_model.load_state_dict(torch.load('./best_model.pth'))\n",
    "best_model.eval()\n",
    " \n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    " \n",
    "# Split prediction into voltage magnitudes and angles\n",
    "n_buses = len(net.bus)\n",
    "V_mag_pred = output[:n_buses].numpy()\n",
    "V_ang_pred = output[n_buses:].numpy()\n",
    " \n",
    "# Run power flow to ensure internal data is available\n",
    "# pp.runpp(net, calculate_voltage_angles=True)\n",
    "pp.runpp(net,\n",
    "         init=\"auto\",\n",
    "         init_vm_pu=V_mag_pred,\n",
    "         init_va_degree=V_ang_pred,\n",
    "         max_iteration=50,\n",
    "         tolerance_mva=1e-5)\n",
    " \n",
    "# Get reference values from the network\n",
    "V_mag_ref = net.res_bus.vm_pu.values\n",
    "V_ang_ref = net.res_bus.va_degree.values\n",
    " \n",
    "print(\"Predicted voltage magnitudes:\", V_mag_pred)\n",
    "print(\"Predicted voltage angles:\", V_ang_pred)\n",
    "print(\"Reference voltage magnitudes:\", V_mag_ref)\n",
    "print(\"Reference voltage angles:\", V_ang_ref)\n",
    "print(\"Iterations:\", net._ppc[\"iterations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7791d-2e2b-4a60-ad32-a2f880ccabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
